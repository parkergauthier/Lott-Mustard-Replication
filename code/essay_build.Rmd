---
title: "Lott & Mustard Extension"
author: "Parker Gauthier"
date: "4/15/2022"
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{placeins}
---

# Lott and Mustard Replication Excercise

## Introduction

  For years the relationship between crime and gun laws has been a topic of significant contention in the United States. Some argue that restricting gun ownership will deter gun violence, while those on the other end of the aisle believe in quite the opposite. Researchers John Lott and David Mustard aimed to clear up this argument in their paper, "Crime, Deterrence, and Right-to-Carry Concealed Handguns."  The authors attempt to tackle this problem by analyzing the effects of concealed carry laws on various crime rates using econometric models aimed at inferring causality.  The authors conclude that when states give their citizens the right to carry a concealed firearm, violent crime rates decline without a significant increase in accidental gun deaths.  Their findings are  intriguing, but were their methods sound?
```
```
  The goal of the analysis below will be to assess the models used by Lott and Mustard and look to how they stack up to contemporary causal inference methods.  We will look at the same data used by the researchers and first attempt to replicate their results.  We will then used other predictive models to see if we see the same effects depicted by the researchers.  Ultimately, we will assess what methods are the most effective in determining causal effects and highlight the implications of using a faulty model.
  
## Background and Economic Theory



```{r include=FALSE}

#loading packages

if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

librarian::shelf( 
  cran_repo = "https://cran.microsoft.com/",
  ask = FALSE,
  stats, 
  here,
  kableExtra,
  ggthemes,
  tidyverse,
  lubridate,
  haven,
  lmtest,
  gganimate,
  gapminder,
  stargazer,
  snakecase,
  mosaic,
  dplyr,
  esquisse,
  plotly,
  modelr,
  rsample,
  caret,
  foreach,
  parallel,
  gamlr,
  glmnet,
  rpart,
  rpart.plot,
  rsample,
  randomForest,
  gbm,
  pdp,
  ggmap,
  devtools,
  estimatr,
  plm,
  bacondecomp,
  TwoWayFEWeights,
  fixest,
  glue,
  did,
  tinytex
)

here::i_am("code/include.R")
```


```{r include=FALSE}
#Reading in the data

Lott = read_dta(here("data/UpdatedStateLevelData-2010.dta"))

```
In their paper, Lott and Mustard look to states where concealed carry is legal and attempt to compare them to states where it is not.  Their goal was to show that the threat of a civilian carrying a firearm deterred crime.  The theory here is that criminals will be less inclined to risk committing a crime the greater the possibility their victims may be armed. They tackled this question by collecting data from the Federal Bureau of Investigation's Uniform Crime Report showing various crime rates between 1977 and 1992.  They also compiled census data to get demographic information for each state.  For each county in a state, there was information on the total instances of violent crimes such as murder, rape and assault. There were also metrics on property crimes like larceny, auto-theft, and burglary to see if criminals would substitute to less violent crimes. The demographic information gave insights on the population's size and makeup, allowing them to control their outcomes for these factors.  Furthermore, each state was coded with a dummy variable indicating "Shall Issue" for what years concealed carry was legal.  Below displays this rollout of concealed carry laws by state for those who legalized at some point prior or during the period:
```
```
\FloatBarrier
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, results='asis'}
knitr::opts_chunk$set(echo = TRUE)
# Table showing rollout

# making data frame for states with year treated
treat_table =  Lott %>%
  select(state, shalll, year) %>%
  group_by(state) %>% 
  filter(shalll == 1 , year >= 1977 & year<=1992) %>%
  summarise(treatment_year = min(year)) %>% 
  mutate(treatment_year = ifelse(treatment_year <= 1977, yes = "Treated Entire Period", no = treatment_year), order = ifelse(treatment_year == "Treated Entire Period", yes = 1, no = treatment_year)) %>%  
  arrange(order) %>% 
  select(-order)


knitr::kable(treat_table, col.names = c("State Name", "Year Treated"), caption = "Rollout", format.args = list(float = FALSE)) %>% 
  kable_styling(c("bordered","condensed"),
                latex_options = c("hold_position"))
```
\FloatBarrier


```
```
By analyzing this rollout, the researchers were able to look at how crime trends changed due to the legalization of concealed carry.  They did this by utilizing a Two Way Fixed Effects research approach, comparing states that were treated with the concealed carry dummy to those who were not for each year. The goal here is to measure the average treatment effect of "Shall Issue" on the various crime rates.  Their ultimate findings from their least squares regressions were that the allowance of concealed carry decreased violent crime rates and did not increase accidental gun deaths.  They also argue that criminals did not significantly switch to more discrete, non-confrontational crimes such as larceny or auto-theft.  They claim that the social gains from legalizing are substantial, that the monetary gain from all states legalizing would be at least $5.74 billion.

## Data

The data Lott & Mustard acquired contained a myriad of information on the crime rates for each of the 50 states between the years 1970 and 1992.  The key outcomes of interest are the rates per 100,000 of violent crime, property crime, murder, rape, robbery, burglary, larceny, and auto-theft.  Below displays a table of summary statistics for these crime rates along with the arrest rates for these crimes:
```
```
\FloatBarrier
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, results='asis'}

# Table with summary statistics
Lott_select = Lott %>% 
  select(state, year, aovio, aopro, aomur, aorap, aorob, aobur, aolar, aoaut, ratvio, ratpro, ratmur, ratrap, rataga, ratrob, rataut, ratbur, ratlar)

namies = c("Arest Rates - Violent Crime", "Property Crime", "Murder", "Rape", "Robbery", "Burglary", "Larceny", "Auto Theft", "Crime Rates - Violent Crime", "Property Crime", "Murder", "Rape", "Agravated Assault", "Robbery","Auto Theft", "Burglary", "Larceny")

sum_tab_sd = Lott_select %>% 
  select(year, aovio, aopro, aomur, aorap, aorob, aobur, aolar, aoaut, ratvio, ratpro, ratmur, ratrap, rataga, ratrob, rataut, ratbur, ratlar) %>%
  filter(year >= 1977 & year <= 1992) %>% 
  select(-year) %>% 
  summarise_all(sd, na.rm = TRUE) %>%
  round(2) %>% 
  t()

sum_tab_mean = Lott_select %>% 
  select(year, aovio, aopro, aomur, aorap, aorob, aobur, aolar, aoaut, ratvio, ratpro, ratmur, ratrap, rataga, ratrob, rataut, ratbur, ratlar) %>%
  filter(year >= 1977 & year <= 1992) %>% 
  select(-year) %>% 
  summarise_all(mean, na.rm = TRUE) %>% 
  round(2) %>% 
  t() %>% 
  cbind(sum_tab_sd)


row.names(sum_tab_mean) = namies

kable(sum_tab_mean, col.names = c("Mean", "Sd"), caption = "Summary Statistics", format.args = list(float = FALSE))%>% 
  kable_styling(c("bordered","condensed"),
                latex_options = c("hold_position"))
```
\FloatBarrier
```
```
The data also includes various demographic information such as population and percent population by race and sex.  This data is originally displayed at the county level, but through some data wrangling we will consolidate it to reflect state level information.


## Empircal Model and Estimation

### Twoway Fixed Effects vs Callaway-Sant'anna

  The first models we will look at will be Twoway Fixed Effects and Callaway-Sant'anna estimation.  Twoway Fixed Effects (the model used by Lott & Mustard) is a difference in differences approach to inferring causality between variables.  The model does so by looking at a the differences in relationships before and after a treatment (in our case this is the "Shall Issue" dummy variable) for treated individuals, and then compares these differences to those that are not treated.  This method unfortunately has issues when there is differential timing between treated groups which is the case in Lott & Mustard's study, suggesting that their estimations may be slightly flawed.  The Callaway-Sant'anna approach attempts to fix this issue by properly weighting the different groups (states treated in a particular year) by the amount of time they are treated. Below displays the different estimations by the two models:
  
```{r include = FALSE}
#TWFE
Lott_mutate = Lott %>% 
  filter(year >= 1977 & year <= 1992) %>% 
  mutate(lvio = log(ratvio), lmur = log(ratmur), laga = log(rataga), lrob = log(ratrob), lpro = log(ratpro), lbur = log(ratbur), llar = log(ratlar), laut = log(rataut)) %>% group_by(state) %>% 
  mutate(treated = max(shalll))

treated_table = Lott_mutate %>% 
  group_by(state) %>% 
  summarize(count = sum(shalll)) %>% 
  mutate(treatment_year = ifelse(count > 0, 1992 + 1 -count,0))


Lott_treat = merge(treated_table, Lott_mutate, by = "state")

y_vars = c("lvio", "lpro", "lmur", "lrap", "laga", "lrob", "lbur", "llar", "laut")

ao_vars = c("aovio", "aopro", "aomur", "aorap", "aoaga", "aorob", "aobur", "aolar", "aoaut")

len_ = length(ao_vars)

foreach(y = 1:len_) %do% {
  y_var = y_vars[y]
  ao_var = ao_vars[y]
  fixie = "| state + year"
  
  specfication = as.formula(paste0(y_var, "~shalll + density + rpcpi + rpcim + rpcui + rpcrpo + ppwm1019 + ppwm2029 + ppwm3039 + ppwm4049 + ppwm5064 + ppwf1019 + ppwf2029 + ppwf3039 + ppwf4049 + ppwf5064 + ppbm1019 + ppbm2029 + ppbm3039 + ppbm4049 + ppbm5064 + ppbf1019 + ppbf2029 + ppbf3039 + ppbf4049 + ppbf5064 +ppnm1019 + ppnm2029 + ppnm3039 + ppnm4049 + ppnm5064 + ppnf1019 + ppnf1019 + ppnf2029 + ppnf3039 + ppnf4049 + ppnf5064 +", ao_var, fixie))
  
  model_fe = feols(fml = specfication, data = Lott_treat)

  model_names = paste("twfe", y_var, sep = "_")
  assign(model_names, model_fe)
  
  fname_twfe = paste0(model_names, ".RDs")
  save(file = file.path(here(), "output", "models", fname_twfe), list = "model_fe")
}
```



```{r include = FALSE}
#bacon decomp

foreach(y=1:len_) %do% {
  y_var = y_vars[y]
  
  bacon_specs = as.formula(paste(y_var, "shalll", sep = " ~ "))

  bacon_model = bacon(bacon_specs,
                    data = Lott_treat,
                    id_var = "fipsstat",
                    time_var = "year") %>% 
    group_by(type) %>% 
    summarise(av_estimate = weighted.mean(estimate, weight),
              weight = sum(weight))
  
  bacon_names = paste("bacon", y_var, sep = "_")
  assign(bacon_names, bacon_model)
  
  fname_bacon = paste0(bacon_names, ".RDs")
  save(file = file.path(here(), "output", "models", fname_bacon), list = "bacon_model")
}

```


```{r include=FALSE}
#Calloway Sant'anna
foreach(y=1:len_) %do% {
  y_var = y_vars[y]
  ao_var = ao_vars[y]

  cs_specs = as.formula(paste("~ +", ao_var))

  att_sa <- att_gt(yname = y_var, # LHS variable
               tname = "year", # time variable
               idname = "fipsstat", # id variable
               gname = "treatment_year", # first treatment period variable
               data = Lott_treat, # data
               xformla = cs_specs, # no covariates
               est_method = "dr", # "dr" is doubly robust. "ipw" is inverse probability weighting. "reg" is regression
               control_group = "notyettreated", # set the comparison group which is either "nevertreated" or "notyettreated" 
               bstrap = TRUE, # if TRUE compute bootstrapped SE
               biters = 1000, # number of bootstrap iterations
               print_details = FALSE, # if TRUE, print detailed results
               clustervars = "fipsstat", # cluster level
               panel = TRUE) # whether the data is panel or repeated cross-sectional


  cs_model = aggte(att_sa, type = "group", balance_e = TRUE, na.rm = TRUE)
  
  cs_names = paste("CS", y_var, sep = "_")
  assign(cs_names, cs_model)
  
  fname_cs = paste0(cs_names, ".RDs")
  save(file = file.path(here(), "output", "models", fname_cs), list = "cs_model")
}

```


```{r include = FALSE}
# Sun and Abraham

foreach(y = 1:len_) %do% {
  y_var = y_vars[y]
  ao_var = ao_vars[y]
  
  sa_spec = as.formula(paste0(y_var, "~ sunab(treatment_year, year) + density + rpcpi + rpcim + rpcui + rpcrpo + ppwm1019 + ppwm2029 + ppwm3039 + ppwm4049 + ppwm5064 + ppwf1019 + ppwf2029 + ppwf3039 + ppwf4049 + ppwf5064 + ppbm1019 + ppbm2029 + ppbm3039 + ppbm4049 + ppbm5064 + ppbf1019 + ppbf2029 + ppbf3039 + ppbf4049 + ppbf5064 +ppnm1019 + ppnm2029 + ppnm3039 + ppnm4049 + ppnm5064 + ppnf1019 + ppnf1019 + ppnf2029 + ppnf3039 + ppnf4049 + ppnf5064 +", ao_var))

  sa_model = feols(fml = sa_spec,
                 data = Lott_treat,
                 subset = ~ year < 1992,
                 vcov = ~fipsstat + year)

  sa_names = paste("SA", y_var, sep = "_")
  assign(sa_names, sa_model)
  
  fname_sa = paste0(sa_names, ".RDs")
  save(file = file.path(here(), "output", "models", fname_sa), list = "sa_model")
}
```

```{r warning=FALSE, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
effects = foreach(y = 1:len_, .combine = rbind) %do% {
  y_var = y_vars[y]
  
  twfe_name = paste("twfe", y_var, sep = "_")
  fname_twfe = paste0(twfe_name, ".RDs")

  
  cs_name = paste("CS", y_var, sep = "_")
  fname_cs = paste0(cs_name, ".RDs")
  
  sa_name = paste("SA", y_var, sep = "_")
  fname_sa = paste0(sa_name, ".RDs")
  
  load(file.path(here(), "output", "models", fname_twfe))
  load(file.path(here(), "output", "models", fname_cs))
  load(file.path(here(), "output", "models", fname_sa))

  
  estimates = c(y_var, model_fe[["coefficients"]][["shalll"]], cs_model[["overall.att"]])
  se = c(y_var, model_fe[["se"]][["shalll"]], cs_model[["overall.se"]])
  p = c(y_var, model_fe[["coeftable"]][["Pr(>|t|)"]][1], cs_model[["overall.se"]])
  
  rbind(estimates, se)
} %>% 
  as.data.frame()

colnames(effects) = c("Target Variable (Log)", "TWFE", "Calloway_SantAnna")
rownames(effects) = NULL

effects$TWFE = round(as.numeric(effects$TWFE), 4)
effects$Calloway_SantAnna = round(as.numeric(effects$Calloway_SantAnna), 4)

effects = effects %>% 
  mutate("Target Variable (Log)" = ifelse(row_number()%%2 == 0, "", c("Rate of Violent Crime", "Murder Rate", "Aggravated Assault Rate", "Burglary Rate", "Auto Crime Rate", "Property Crime Rate", "Rape Rate", "Robbery Rate", "Larceny Rate"))) %>% 
  mutate(TWFE = ifelse(row_number()%%2 == 0, paste0("(", TWFE, ")"), TWFE), 
         Calloway_SantAnna = ifelse(row_number()%%2 == 0, paste0("(",Calloway_SantAnna, ")"),Calloway_SantAnna)) 

stargazer(effects,
          type= "latex",
          summary = FALSE,
          rownames = FALSE,
          header = FALSE)
```

\FloatBarrier

We can see that Twoway Fixed Effects reflects a significant negative relationship between violent crime and "Shall Issue," suggesting that concealed carry legalization decreases the rate of violent crime.  In contrast, the Calloway-Sant'anna estimator is not significantly different from zero.  By utilizing the weights given to us by this specification, we can see that our outcomes change quite drastically in this regard. It would appear that only the effect on Auto Crime is significantly negative.  Ultimately, this shows that the estimates reported in Lott & Mustard's paper are quite possibly not reflective of reality.


### Bacon Decomposition

The next method we will use is the Bacon Decomposition.  Below is a table of estimates showing aggregated 2x2 effects for each outcome.  The two groups here are "Earlier" and "Later," grouping together observation that were treated earlier in the period vs later. The table then depicts the average estimate from "Shall Issue," comparing treated group the not yet treated groups. Important to note is that the estimates for "Later vs Earlier" are based on counterfactuals as opposed to an untreated group since later treated groups have no yet-to-be-treated groups to be compared to.  This, paired with the length of the study and the differential timing, causes the estimates for "Earlier vs Later" to be weighted approximately three times higher than "Later vs Earlier."  We can see that the estimates for causal effects in this table are generally small, and the contrast in signage for some of the variable may further indicate insignificant effects.    
```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
baconator = foreach(y = 1:len_, .combine = rbind) %do% {
  y_var = y_vars[y]
  
  bacon_name = paste("bacon", y_var, sep="_")
  fname_bacon = paste0(bacon_name, ".RDs")
  
  load(file.path(here(), "output", "models", fname_bacon))
  
  estimates = c(y_var, bacon_model[1,])
  estimates2 = c(y_var, bacon_model[3,])
  
  rbind(estimates, estimates2)
} %>% 
  data.frame()

colnames(baconator) = c("Treated_Variable_(Log)", "Type", "Average_Estimate", "Weight")
rownames(baconator) = NULL

baconator = baconator %>% 
  mutate(`Treated_Variable_(Log)` = ifelse(row_number()%%2 == 0, "", c("Rate of Violent Crime", "Murder Rate", "Aggravated Assault Rate", "Burglary Rate", "Auto Crime Rate", "Property Crime Rate", "Rape Rate", "Robbery Rate", "Larceny Rate")))


stargazer(baconator, type = "latex", summary = FALSE, rownames = FALSE, header = FALSE)
```

\FloatBarrier

### Event Study
Finally, we will look at the Sun & Abraham Event Study.  This method plots the interaction weighted estimator of the treatment effect on our variables of interest over the time of treatment.  This estimator is derived from the difference in crime rates between the different groups based on the time of treatment.  It gives us a visual as to where the treatment effect is significant and how the treatment affected our outcome variables.  We will see that for most of the crime variables "Shall Issue" initially decreases the crime rate but this is followed by an uptick in the rate.  In the example of violent crime, the effect is significantly negative in the first years after the treatment but appears to go up with time (granted the positive effects are statistically insignificant, but this is still a movement away from the initial negative impact).  Furthermore, pretrends in the data do not seem to hold post-treatment.  The treatment appears to cause significant swings in the estimator.  Additionally, parallel trends would likely not hold in this scenario as different groups may experience different crime rates post treatment due to other factors than "Shall Issue."  
```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,1))
iplot(SA_lvio,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Violent Crime")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
iplot(SA_lpro,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Property Crime")
iplot(SA_lmur,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Murder")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
iplot(SA_lrap,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Rape")
iplot(SA_laga,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Aggravated Assault")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
iplot(SA_lrob,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Robbery")
iplot(SA_lbur,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Burglary")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
iplot(SA_llar,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Larceny")
iplot(SA_laut,sep =.5,ref.line = -1,
      xlab = 'Time to treatment',
      main = 'Staggered treatment',
      sub = "Auto Theft")
```

\FloatBarrier

 
## Conclusion

Ultimately, this replication has given us insights into how the Twoway Fixed Effects design can give us misleading results. When using more contemporary methods, the impact of "Shall Issue" on our outcome variables depreciates in statistical significance and in some cases even switches sign.  Because of this, Lott & Mustard's findings are shown to be flawed and their conclusions require revisiting. When looking to build a better case for inferring causality with differential timing, the Calloway-Sant'Anna, Bacon, and Sun and Abraham designs give us much more reliable estimators.  With that said, these models still may have their flaws when working with this kind of data. "Shall Issue" laws are not the same for each state which may effect who is allowed to have a concealed firearm.  Since this treatment is not the same across all units in our population, this is not a Stable Unit Treatment Variables Variable. Other estimation methods may be necessary to extract the true causal effect from "Shall Issue" on crime rates.